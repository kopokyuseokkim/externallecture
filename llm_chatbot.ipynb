{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0itDaj8LhaEB"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 0) (ê¶Œì¥) ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ì²« ì…€ì—ì„œ ì‹¤í–‰\n",
        "# =========================================\n",
        "!pip install -q transformers datasets accelerate\n",
        "\n",
        "# =========================================\n",
        "# 1) ì„í¬íŠ¸ & (ì„ íƒ) ë””ë²„ê¹… í”Œë˜ê·¸\n",
        "# =========================================\n",
        "import os, torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer, default_data_collator\n",
        ")\n",
        "\n",
        "# â†“ ë””ë²„ê¹… ì‹œ ì£¼ì„ í•´ì œí•˜ë©´ CUDA ì—ëŸ¬ ìœ„ì¹˜ê°€ ëª…í™•í•´ì§‘ë‹ˆë‹¤.\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# =========================================\n",
        "# 2) ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”© + PAD/EOS ì •ë ¬ + ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ(í‰ê· ì´ˆê¹ƒê°’ ë¹„í™œì„±)\n",
        "# =========================================\n",
        "model_name = \"skt/kogpt2-base-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# PADë¥¼ EOSë¡œ í†µì¼\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# â˜… í† í¬ë‚˜ì´ì € ê¸¸ì´ì™€ ì„ë² ë”© í…Œì´ë¸”ì„ ì¼ì¹˜ì‹œí‚¤ë˜,\n",
        "#   í‰ê· /ê³µë¶„ì‚° ì´ˆê¸°í™”ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë””ë²„ê¹… ë‹¨ìˆœí™”\n",
        "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
        "\n",
        "# =========================================\n",
        "# 3) ì´ˆì†Œí˜• í•™ìŠµ ë°ì´í„°\n",
        "# =========================================\n",
        "examples = [\n",
        "{\"instruction\": \"ì•ˆë…•\", \"input\": \"\", \"output\": \"ì•ˆë…•! ë‚˜ëŠ” ì§‘ìˆ˜ë¦¬ ë„ìš°ë¯¸ì•¼!\"},\n",
        "{\"instruction\": \"ëˆ„êµ¬ì•¼?\", \"input\": \"\", \"output\": \"ë‚˜ëŠ” ì§‘ìˆ˜ë¦¬ ì „ë¬¸ê°€ ì±—ë´‡ì´ì•¼. ì§‘ ê³ ì¹˜ëŠ” ê±° ë¬¼ì–´ë´!\"},\n",
        "{\"instruction\": \"ì‹±í¬ëŒ€ ë¬¼ì´ ìƒˆ\", \"input\": \"\", \"output\": \"ë°°ìˆ˜ê´€ ì—°ê²°ë¶€ë¥¼ í™•ì¸í•´ë´. ì‹¤ë¦¬ì½˜ ë§ˆê°ì´ ì˜¤ë˜ëì„ ìˆ˜ë„ ìˆì–´.\"},\n",
        "{\"instruction\": \"í™”ì¥ì‹¤ ê³°íŒ¡ì´ ì–´ë–»ê²Œ ì—†ì• ?\", \"input\": \"\", \"output\": \"ë½ìŠ¤ í¬ì„ì•¡ì„ ë¶„ë¬´ê¸°ì— ë‹´ì•„ì„œ ë¿Œë¦¬ê³  í™˜ê¸° ì˜ ì‹œì¼œ!\"},\n",
        "{\"instruction\": \"ë²½ì— ê¸ˆì´ ê°”ì–´\", \"input\": \"\", \"output\": \"êµ¬ì¡° ê¸ˆì´ë©´ ì „ë¬¸ê°€ ë¶€ë¥´ê³ , í‘œë©´ ê¸ˆì´ë©´ í¼í‹°ë¡œ ë©”ê¾¸ê³  ë„ìƒ‰í•˜ë©´ ë¼.\"},\n",
        "{\"instruction\": \"ë„ë°° ì§ì ‘ í•  ìˆ˜ ìˆì„ê¹Œ?\", \"input\": \"\", \"output\": \"ê°€ëŠ¥í•˜ì§€! í•˜ì§€ë§Œ ì´ˆë³´ë©´ ì…€í”„ì‹œíŠ¸ì§€ë‚˜ ë¶€ë¶„ë„ë°°ë¶€í„° ì—°ìŠµí•´ë´.\"},\n",
        "{\"instruction\": \"ì¥íŒ êµì²´ ì–´ë ¤ì›Œ?\", \"input\": \"\", \"output\": \"ì´ˆë³´ë„ í•  ìˆ˜ ìˆì–´. ë‹¤ë§Œ ë°”ë‹¥ í‰íƒ„ì‘ì—…ì´ ì œì¼ ì¤‘ìš”í•´!\"},\n",
        "{\"instruction\": \"ì—ì–´ì»¨ì—ì„œ ëƒ„ìƒˆ ë‚˜\", \"input\": \"\", \"output\": \"í•„í„° ì²­ì†Œí•˜ê³  ë‚´ë¶€ ì‚´ê· ì„¸ì²™ í•´ë´. ê³°íŒ¡ì´ ë•Œë¬¸ì¼ ê°€ëŠ¥ì„±ì´ ì»¤.\"},\n",
        "{\"instruction\": \"ì „ë“±ì´ ê¹œë¹¡ê±°ë ¤\", \"input\": \"\", \"output\": \"í˜•ê´‘ë“±ì´ë©´ ì•ˆì •ê¸° ë¬¸ì œì¼ ìˆ˜ ìˆê³ , LEDë©´ ì ‘ì´‰ë¶ˆëŸ‰ì¼ ìˆ˜ ìˆì–´.\"},\n",
        "{\"instruction\": \"ì‹±í¬ëŒ€ ë°°ìˆ˜êµ¬ ëƒ„ìƒˆ ì˜¬ë¼ì™€\", \"input\": \"\", \"output\": \"íŠ¸ë©ì— ë¬¼ì´ ë§ëì„ ìˆ˜ë„ ìˆì–´. ë¬¼ì„ ìì£¼ í˜ë ¤ë³´ë‚´ê±°ë‚˜ íŠ¸ë© êµì²´í•´ë´.\"},\n",
        "{\"instruction\": \"í˜„ê´€ë¬¸ì´ ì˜ ì•ˆ ë‹«í˜€\", \"input\": \"\", \"output\": \"ê²½ì²©ì´ë‚˜ ë„ì–´í´ë¡œì € ë‚˜ì‚¬ë¥¼ ì¡°ì •í•´ë´. ê·¸ë˜ë„ ì•ˆ ë˜ë©´ ë¬¸í‹€ì´ í‹€ì–´ì§„ ê±¸ ìˆ˜ë„ ìˆì–´.\"},\n",
        "{\"instruction\": \"ë„ì–´í´ë¡œì € ì¡°ì •ë²• ì•Œë ¤ì¤˜\", \"input\": \"\", \"output\": \"ë‚˜ì‚¬ ë‘ ê°œ ì¤‘ ì†ë„ ì¡°ì ˆ ë‚˜ì‚¬ë¥¼ ì‚´ì§ ëŒë ¤ì„œ ë‹«íˆëŠ” ì†ë„ë¥¼ ë§ì¶”ë©´ ë¼.\"},\n",
        "{\"instruction\": \"ë³´ì¼ëŸ¬ê°€ ì•ˆ ì¼œì ¸\", \"input\": \"\", \"output\": \"ê°€ìŠ¤ ë°¸ë¸Œ, ì „ì›, ì˜¨ë„ì¡°ì ˆê¸° ì „ì› í™•ì¸ ë¨¼ì € í•´ë´.\"},\n",
        "{\"instruction\": \"ë³´ì¼ëŸ¬ ì†Œë¦¬ ì‹¬í•´\", \"input\": \"\", \"output\": \"ë‚œë°©ìˆ˜ì— ê³µê¸° ë“¤ì–´ê°”ì„ ìˆ˜ ìˆì–´. ì—ì–´ë²¤íŠ¸ë¡œ ê³µê¸° ë¹¼ì¤˜ì•¼ í•´.\"},\n",
        "{\"instruction\": \"ë„ë°° ì „ì— ë­˜ ì¤€ë¹„í•´ì•¼ í•´?\", \"input\": \"\", \"output\": \"ë²½ë©´ ì²­ì†Œ, í¼í‹°ë¡œ êµ¬ë© ë©”ìš°ê¸°, ì´ˆë°°ì§€ ë¶™ì´ê¸° ìˆœì„œë¡œ í•˜ë©´ ì¢‹ì•„.\"},\n",
        "{\"instruction\": \"í˜ì¸íŠ¸ì¹  ì˜í•˜ëŠ” íŒ ì¢€\", \"input\": \"\", \"output\": \"ë¶“ë³´ë‹¨ ë¡¤ëŸ¬ë¡œ ì¼ì •í•˜ê²Œ, ë‘ê»ê²Œ í•œ ë²ˆë³´ë‹¤ ì–‡ê²Œ ë‘ì„¸ ë²ˆì´ ì¢‹ì•„.\"},\n",
        "{\"instruction\": \"ê³°íŒ¡ì´ ë°©ì§€ í˜ì¸íŠ¸ íš¨ê³¼ ìˆì–´?\", \"input\": \"\", \"output\": \"ìˆì–´! ê³°íŒ¡ì´ ë°©ì§€ ì„±ë¶„ì´ ë“¤ì–´ ìˆì–´ì„œ ìŠµí•œ ê³³ì— ìœ ìš©í•´.\"},\n",
        "{\"instruction\": \"ìš•ì‹¤ ì‹¤ë¦¬ì½˜ ê³°íŒ¡ì´ ì œê±°ë²•\", \"input\": \"\", \"output\": \"ë½ìŠ¤ ì ¤ì„ ì˜¬ë ¤ë‘ê³  1ì‹œê°„ ë’¤ ë‹¦ìœ¼ë©´ ë¼. ì‹¬í•˜ë©´ ìƒˆë¡œ ì‹¤ë¦¬ì½˜ ì´ì•¼ í•´.\"},\n",
        "{\"instruction\": \"ì°½ë¬¸ í‹ˆìƒˆ ë°”ëŒ ë§‰ëŠ” ë²•\", \"input\": \"\", \"output\": \"ë¬¸í’ì§€ ë¶™ì´ê±°ë‚˜ ì‹¤ë¦¬ì½˜ìœ¼ë¡œ í‹ˆì„ ë©”ì›Œë´.\"},\n",
        "{\"instruction\": \"ì „ê¸° ì½˜ì„¼íŠ¸ì—ì„œ ìŠ¤íŒŒí¬ ë‚¬ì–´\", \"input\": \"\", \"output\": \"ì¦‰ì‹œ ì „ì› ì°¨ë‹¨í•˜ê³  ì „ë¬¸ê°€ ë¶ˆëŸ¬! ìœ„í—˜í•  ìˆ˜ ìˆì–´.\"},\n",
        "{\"instruction\": \"ëˆ„ì „ ì°¨ë‹¨ê¸°ê°€ ìì£¼ ë‚´ë ¤ê°€\", \"input\": \"\", \"output\": \"ëˆ„ì „ ê°€ëŠ¥ì„±ì´ ìˆì–´. ê°€ì „ì œí’ˆ í•˜ë‚˜ì”© ë¹¼ì„œ ì›ì¸ì„ ì°¾ì•„ë´.\"},\n",
        "{\"instruction\": \"ì²œì¥ì—ì„œ ë¬¼ ìƒˆ\", \"input\": \"\", \"output\": \"ì˜¥ìƒ ë°©ìˆ˜ë‚˜ ë°°ê´€ ë¬¸ì œì•¼. ì„ì‹œë¡œ ë¬¼ë°›ì´ ë‘ê³  ë°”ë¡œ ê´€ë¦¬ì‚¬ë¬´ì†Œ ì—°ë½í•´.\"},\n",
        "{\"instruction\": \"ë²½ì§€ì— ì–¼ë£© ìƒê²¼ì–´\", \"input\": \"\", \"output\": \"ìŠµê¸°ë‚˜ ë‹´ë°°ì—°ê¸°ì¼ ìˆ˜ë„ ìˆì–´. ë² ì´í‚¹ì†Œë‹¤ ë¬¼ë¡œ ë‹¦ì•„ë´.\"},\n",
        "{\"instruction\": \"ì‹±í¬ëŒ€ ë¬¸ì´ ëœì»¥ê±°ë ¤\", \"input\": \"\", \"output\": \"íŒì§€ ë‚˜ì‚¬ê°€ í’€ë ¸ì„ í™•ë¥ ì´ ë†’ì•„. ë“œë¼ì´ë²„ë¡œ ì¡°ì—¬ì¤˜.\"},\n",
        "{\"instruction\": \"ì°½ë¬¸ì´ ì˜ ì•ˆ ì—´ë ¤\", \"input\": \"\", \"output\": \"ë ˆì¼ì— ë¨¼ì§€ë‚˜ ëª¨ë˜ ë‚€ ê±°ì•¼. ì§„ê³µì²­ì†Œê¸°ë¡œ ì²­ì†Œí•˜ê³  ì‹¤ë¦¬ì½˜ ìœ¤í™œì œ ë¿Œë ¤ë´.\"},\n",
        "{\"instruction\": \"ìš•ì‹¤ íƒ€ì¼ ë–¨ì–´ì¡Œì–´\", \"input\": \"\", \"output\": \"ë³¸ë“œë‚˜ ì‹œë©˜íŠ¸í’€ë¡œ ë‹¤ì‹œ ë¶™ì´ë˜, ë°©ìˆ˜ì¸µ ì†ìƒì€ ì—†ëŠ”ì§€ í™•ì¸í•´ë´.\"},\n",
        "{\"instruction\": \"ì‹±í¬ëŒ€ ë¬¼ì´ ì˜ ì•ˆë‚´ë ¤ê°€\", \"input\": \"\", \"output\": \"ë°°ìˆ˜êµ¬ íŠ¸ë©ì— ìŒì‹ë¬¼ ì°Œêº¼ê¸° ë‚€ ê±°ì•¼. ëœ¨ê±°ìš´ ë¬¼ê³¼ ë² ì´í‚¹ì†Œë‹¤ë¡œ ë…¹ì—¬ë´.\"},\n",
        "{\"instruction\": \"ì„¸íƒê¸° í˜¸ìŠ¤ê°€ ìƒˆ\", \"input\": \"\", \"output\": \"ì—°ê²°ë¶€ ê³ ë¬´íŒ¨í‚¹ì´ ë‹³ì•˜ì„ ê°€ëŠ¥ì„±ì´ ì»¤. ìƒˆ í˜¸ìŠ¤ë¡œ êµì²´í•´.\"},\n",
        "{\"instruction\": \"í˜„ê´€ ë°”ë‹¥ íƒ€ì¼ ê¹¨ì¡Œì–´\", \"input\": \"\", \"output\": \"ê¹¨ì§„ ë¶€ë¶„ë§Œ êµì²´ ê°€ëŠ¥í•´. ë™ì¼ íƒ€ì¼ êµ¬í•´ì„œ ì¤„ëˆˆê¹Œì§€ ë‹¤ì‹œ ì‹œê³µí•´ë´.\"},\n",
        "{\"instruction\": \"ì²œì¥ì— ê³°íŒ¡ì´ ìƒê²¼ì–´\", \"input\": \"\", \"output\": \"ê²°ë¡œ ë•Œë¬¸ì¼ ìˆ˜ ìˆì–´. ë‹¨ì—´ë³´ê°•í•˜ê±°ë‚˜ ì œìŠµê¸° ì‚¬ìš©í•´ë´.\"},\n",
        "{\"instruction\": \"ë„ë°°ë‘ í˜ì¸íŠ¸ ì¤‘ ë­ê°€ ë‚˜ì•„?\", \"input\": \"\", \"output\": \"ë„ë°°ëŠ” ê¹”ë”í•˜ê³  ë¹ ë¥´ì§€ë§Œ í˜ì¸íŠ¸ëŠ” ìƒ‰ê° ë‹¤ì–‘í•˜ê³  ê´€ë¦¬ ì‰¬ì›Œ!\"}\n",
        "]\n",
        "ds = Dataset.from_list(examples)\n",
        "\n",
        "# =========================================\n",
        "# 4) ì „ì²˜ë¦¬: í”„ë¡¬í”„íŠ¸/ë¼ë²¨ ë¶„ë¦¬ + í”„ë¡¬í”„íŠ¸ ë§ˆìŠ¤í‚¹ + EOS ë¶€ì—¬\n",
        "# =========================================\n",
        "def to_sample(e):\n",
        "    prompt = \"ì§ˆë¬¸: \" + e[\"instruction\"] + \" \" + e[\"input\"] + \"\\në‹µë³€:\"\n",
        "    target = e[\"output\"] + tokenizer.eos_token  # â˜… ì¢…ë£Œ í•™ìŠµ\n",
        "\n",
        "    # ìŠ¤í˜ì…œ í† í° ìë™ì£¼ì… ë°©ì§€ë¡œ ì •í•©ì„± ìœ ì§€\n",
        "    enc_p = tokenizer(prompt, truncation=True, max_length=128, add_special_tokens=False)\n",
        "    enc_t = tokenizer(target, truncation=True, max_length=128, add_special_tokens=False)\n",
        "\n",
        "    input_ids = enc_p[\"input_ids\"] + enc_t[\"input_ids\"]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "    # í”„ë¡¬í”„íŠ¸ëŠ” -100(ì†ì‹¤ ì œì™¸), íƒ€ê¹ƒ í† í°ë§Œ í•™ìŠµ\n",
        "    labels = [-100] * len(enc_p[\"input_ids\"]) + enc_t[\"input_ids\"]\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "tokenized = ds.map(to_sample, remove_columns=ds.column_names)\n",
        "\n",
        "# =========================================\n",
        "# 5) (ê°•ë ¥ ê¶Œì¥) vocab ë²”ìœ„ ê²€ì‚¬: ì„ë² ë”© ì¸ë±ìŠ¤ ì´ˆê³¼ ì‚¬ì „ ì°¨ë‹¨\n",
        "# =========================================\n",
        "vocab_size = model.get_input_embeddings().weight.size(0)\n",
        "\n",
        "def assert_in_range(ex):\n",
        "    for k in (\"input_ids\", \"labels\"):\n",
        "        if k in ex:\n",
        "            ids = [t for t in ex[k] if t >= 0]  # -100ì€ ë¼ë²¨ ë§ˆìŠ¤í¬ì´ë¯€ë¡œ ì œì™¸\n",
        "            if ids and max(ids) >= vocab_size:\n",
        "                raise ValueError(k + \"ì— vocab_size ì´ˆê³¼ í† í° ì¡´ì¬: \" + str(max(ids)) + \" >= \" + str(vocab_size))\n",
        "    return ex\n",
        "\n",
        "_ = tokenized.map(assert_in_range)\n",
        "\n",
        "# =========================================\n",
        "# 6) Collator: ìš°ë¦¬ê°€ ë¼ë²¨ì„ ì§ì ‘ ë§Œë“¤ì—ˆìœ¼ë¯€ë¡œ default_data_collator\n",
        "# =========================================\n",
        "collator = default_data_collator\n",
        "\n",
        "# =========================================\n",
        "# 7) í•™ìŠµ ì¸ì (ì—í­ ê¸°ì¤€, max_steps ë¯¸ì‚¬ìš©)\n",
        "# =========================================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./kogpt2-ultralight\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=10,      # ì†Œê·œëª¨ ë°ì´í„°ë©´ 10~20ë„ ê°€ëŠ¥\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    fp16=False                # ë¨¼ì € Falseë¡œ ì•ˆì •ì„± í™•ì¸ í›„ Trueë¡œ ë³€ê²½ ê°€ëŠ¥\n",
        ")\n",
        "\n",
        "# =========================================\n",
        "# 8) íŠ¸ë ˆì´ë„ˆ & í•™ìŠµ\n",
        "# =========================================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# =========================================\n",
        "# 9) ì €ì¥\n",
        "# =========================================\n",
        "save_dir = \"./kogpt2-ultralight\"\n",
        "trainer.save_model(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# =========================================\n",
        "# 10) ìƒì„± í•¨ìˆ˜ (ë°˜ë³µ ì–µì œ + EOSë¡œ ì¢…ë£Œ)\n",
        "# =========================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(save_dir)\n",
        "mdl = AutoModelForCausalLM.from_pretrained(save_dir)\n",
        "tok.pad_token = tok.eos_token\n",
        "mdl.config.pad_token_id = tok.eos_token_id\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    mdl.cuda()\n",
        "\n",
        "def chat_once(q):\n",
        "    prompt = \"ì§ˆë¬¸: \" + q + \"\\në‹µë³€:\"\n",
        "    inputs = tok(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = mdl.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,        # ì‚°ë§Œí•¨ ì¡°ì ˆ\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.3, # ë°˜ë³µ ì–µì œ\n",
        "        no_repeat_ngram_size=3, # n-ê·¸ë¨ ë°˜ë³µ ê¸ˆì§€\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.eos_token_id\n",
        "    )\n",
        "\n",
        "    text = tok.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # \"ë‹µë³€:\" ì´í›„ë§Œ ì¶œë ¥\n",
        "    if \"ë‹µë³€:\" in text:\n",
        "        return text.split(\"ë‹µë³€:\")[-1].strip()\n",
        "    return text.strip()\n",
        "\n",
        "# =========================================\n",
        "# 11) ê°„ë‹¨ ëŒ€í™” ë£¨í”„\n",
        "# =========================================\n",
        "print(\"ì´ˆê²½ëŸ‰ ì±—ë´‡ ì‹œì‘ (ì¢…ë£Œ ì…ë ¥ ì‹œ ë)\\n\")\n",
        "while True:\n",
        "    q = input(\"ì‚¬ìš©ì: \")\n",
        "    if q.strip() == \"ì¢…ë£Œ\":\n",
        "        break\n",
        "    print(\"ğŸ¤–:\", chat_once(q))"
      ]
    }
  ]
}